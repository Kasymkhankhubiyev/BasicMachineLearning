{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8XXcE7KhyYkj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "PZcRYgmcNxSa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Meta Algorithm"
      ],
      "metadata": {
        "id": "-Q2K-3c7MCnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
      ],
      "metadata": {
        "id": "THabbwtaMBL4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "PU6aRi7PMSVw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ML 4 course/nsu_train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ML 4 course/nsu_test.csv')"
      ],
      "metadata": {
        "id": "WJB_rja6Mv_f"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавим среднюю цену среди категории"
      ],
      "metadata": {
        "id": "jGrQK7lQM7QH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_price_per_category = []\n",
        "for cat in np.unique(train_data['Category'].values):\n",
        "    # print(cat)\n",
        "    avg_price_per_category.append(train_data[train_data['Category'] == cat]['Average price'].mean())\n",
        "    cat_mean_price = train_data[train_data['Category'] == cat]['Average price'].mean()\n",
        "    train_data.loc[train_data['Category'] == cat, 'Avg_price_per_category'] = cat_mean_price\n",
        "# print(avg_price_per_category)\n",
        "# train_data.head()\n",
        "\n",
        "avg_price_per_category = []\n",
        "for cat in np.unique(test_data['Category'].values):\n",
        "    # print(cat)\n",
        "    avg_price_per_category.append(test_data[test_data['Category'] == cat]['Average price'].mean())\n",
        "    cat_mean_price = test_data[test_data['Category'] == cat]['Average price'].mean()\n",
        "    test_data.loc[test_data['Category'] == cat, 'Avg_price_per_category'] = cat_mean_price"
      ],
      "metadata": {
        "id": "fe9z-dvAM8K6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавим фичу, если средняя цена продажи не выше чем средняя цена по сегменту - 1, иначе 0"
      ],
      "metadata": {
        "id": "CC3R48vjNVHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['avg_price_not_more_than_evg_cat'] = np.where((train_data['Avg_price_per_category'] >= train_data['Average price']), 1, 0)\n",
        "train_data['avg_price_not_more_than_evg_cat'] = train_data['avg_price_not_more_than_evg_cat'].astype('object')\n",
        "\n",
        "test_data['avg_price_not_more_than_evg_cat'] = np.where((test_data['Avg_price_per_category'] >= test_data['Average price']), 1, 0)\n",
        "test_data['avg_price_not_more_than_evg_cat'] = test_data['avg_price_not_more_than_evg_cat'].astype('object')\n",
        "\n",
        "ratings = train_data['Rating'].values\n",
        "num_ratings = [int(rating.split(',')[0]) for rating in ratings]\n",
        "train_data['Rating'] = num_ratings\n",
        "\n",
        "ratings = test_data['Rating'].values\n",
        "num_ratings = [int(rating.split(',')[0]) for rating in ratings]\n",
        "test_data['Rating'] = num_ratings"
      ],
      "metadata": {
        "id": "w9RZUnwLNSuF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "добавим процентное отклонение BasePrice от AveragePrice"
      ],
      "metadata": {
        "id": "OPHW0O79NZR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_price = train_data['Average price'].values\n",
        "base_price = train_data['Base price'].values\n",
        "\n",
        "devitation = np.abs((base_price - avg_price)/avg_price)\n",
        "print(devitation)\n",
        "train_data['AvgPrice_BasePrice_percentage_devitation'] = devitation\n",
        "\n",
        "avg_price = test_data['Average price'].values\n",
        "base_price = test_data['Base price'].values\n",
        "\n",
        "devitation = np.abs((base_price - avg_price)/avg_price)\n",
        "print(devitation)\n",
        "test_data['AvgPrice_BasePrice_percentage_devitation'] = devitation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnLZpDBgNb2a",
        "outputId": "31f3d1d1-f081-4961-bdb9-b40e589e960f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08266153 1.44055069 0.05272727 ... 0.07336695 0.96251378 1.56508219]\n",
            "[1.70381587 1.00814111 0.46786031 ... 0.67283991 0.5625     0.1765061 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['comments_more_than_avg'] = (train_data['Comments'] > 4).astype('object')\n",
        "# pd.crosstab(train_data['comments_more_than_avg'], train_data['Sales'], margins=True)\n",
        "test_data['comments_more_than_avg'] = (test_data['Comments'] > 4).astype('object')"
      ],
      "metadata": {
        "id": "os39K2kCNqyV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deleteId(ds):\n",
        "    return ds.drop(columns='Id')\n",
        "\n",
        "def deleteNaN(train_ds, test_ds, critval):\n",
        "    fullsize = train_ds.shape[0]\n",
        "    new_train = train_ds.copy()\n",
        "    new_test = test_ds.copy()\n",
        "    for feature in new_train.columns:\n",
        "        nulls = new_train[feature].isnull().sum()\n",
        "        percent = nulls / fullsize\n",
        "        # если доля пустых значений превышает critval - столбец не информативен,\n",
        "        # можно его выбросить\n",
        "        if (percent > critval):\n",
        "            new_train = new_train.drop(columns=feature)\n",
        "            new_test = new_test.drop(columns=feature)\n",
        "    return new_train, new_test\n",
        "\n",
        "def convertToNumeric(train_ds, test_ds):\n",
        "    new_train = train_ds.copy()\n",
        "    new_test = test_ds.copy()\n",
        "    LE = LabelEncoder()\n",
        "    for feature in new_train.columns:\n",
        "        if (new_train[feature].dtype == 'object'):\n",
        "            new_train[feature] = LE.fit_transform(new_train[feature])\n",
        "            new_test[feature] = LE.fit_transform(new_test[feature])\n",
        "    return new_train, new_test"
      ],
      "metadata": {
        "id": "eFqFpxIHN2hG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train_data.drop_duplicates()\n",
        "\n",
        "# train = train.drop(columns='Id')\n",
        "train = train.drop(columns='Color')\n",
        "# train = train.drop(columns='Base price')\n",
        "# train = train.drop(columns='Basic Sale Price')\n",
        "train = train.drop(columns='Max price')\n",
        "train = train.drop(columns='Min price')\n",
        "\n",
        "test = test_data.drop(columns='Id')\n",
        "test = test.drop(columns='Color')\n",
        "# test = test.drop(columns='Base price')\n",
        "# test = test.drop(columns='Basic Sale Price')\n",
        "test = test.drop(columns='Max price')\n",
        "test = test.drop(columns='Min price')\n",
        "\n",
        "train, test = deleteNaN(train, test, critval=0.8)\n",
        "train, test = convertToNumeric(train, test)\n",
        "\n",
        "train_y = train['Sales'].values\n",
        "train_X = train.drop(columns='Sales').values\n",
        "val_test = test.values\n",
        "\n",
        "train_x_models, train_x_meta, train_y_models, train_y_meta = train_test_split(train_X, train_y, test_size=0.3, random_state=7)\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(train_x_models, train_y_models, test_size=0.1, random_state=7)  # random_state=98987)\n",
        "train_x_meta, test_x_meta, train_y_meta, test_y_meta = train_test_split(train_x_meta, train_y_meta, test_size=0.1, random_state=7)"
      ],
      "metadata": {
        "id": "RnAt24axN6u5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strategies = ['mean', 'median', 'most_frequent']\n",
        "imputer = SimpleImputer(strategy=strategies[2])\n",
        "trainX = imputer.fit_transform(train_x)\n",
        "testX = imputer.fit_transform(test_x)\n",
        "\n",
        "imputer = SimpleImputer(strategy=strategies[2])\n",
        "trainX_meta = imputer.fit_transform(train_x_meta)\n",
        "testX_meta = imputer.fit_transform(test_x_meta)\n",
        "\n",
        "imputer = SimpleImputer(strategy=strategies[2])\n",
        "val_test_x = imputer.fit_transform(val_test)"
      ],
      "metadata": {
        "id": "vP0o9-GZOAXd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "trainX = scaler.fit_transform(trainX)\n",
        "testX = scaler.transform(testX)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "trainX_meta = scaler.fit_transform(trainX_meta)\n",
        "testX_meta = scaler.transform(testX_meta)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "val_test_x = scaler.fit_transform(val_test_x)"
      ],
      "metadata": {
        "id": "fngBjUTMOCr5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Метрика **SMAPE**"
      ],
      "metadata": {
        "id": "zJmUxNsCOIxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smape(A, F):\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        tmp = 2 * np.abs(F-A) / (np.abs(A) + np.abs(F))\n",
        "    tmp[np.isnan(tmp)] = 0\n",
        "    return np.sum(tmp) / len(tmp) * 100\n",
        "\n",
        "def check_error(preds, gt):\n",
        "    print('SMAPE Error:', smape(np.round(preds), gt))"
      ],
      "metadata": {
        "id": "CHujbRhuOJPR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest"
      ],
      "metadata": {
        "id": "frIrOLwKOR74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "обучим сразу несколько моделей, а потом по их предсказаниям построим регрессор, который приблизит эти значения до нужно уровня"
      ],
      "metadata": {
        "id": "qFt89IJxOTu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainX.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HYJo4K3WyjT",
        "outputId": "d085d42c-3214-4c2a-df28-8d58f6a67c6b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15460, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters = {'criterion': 'squared_error', \n",
        "                  'max_depth': 3000, \n",
        "                  'min_samples_leaf': 7, \n",
        "                  'min_samples_split': 4, \n",
        "                  'n_estimators': 70}  # - submission(8) - 20.89%\n",
        "\n",
        "model1 = RandomForestRegressor(**best_parameters) \n",
        "model1.fit(trainX, train_y)\n",
        "check_error(model1.predict(testX), test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU3EdvV3OQeM",
        "outputId": "bcc4a801-3cd8-44ed-b5bb-d26370a1ebb7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMAPE Error: 25.72874118204595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters = {'criterion': 'squared_error',\n",
        "                  'max_depth': 3000,\n",
        "                  'min_samples_leaf': 7,\n",
        "                  'min_samples_split': 4,\n",
        "                  'n_estimators': 80}  # - submission(8) - 20.87%\n",
        "\n",
        "# random_forest = RandomForestRegressor(**rf_gs_model.best_params_)\n",
        "model2 = RandomForestRegressor(**best_parameters) \n",
        "model2.fit(trainX, train_y)\n",
        "check_error(model2.predict(testX), test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVEnUGHnOhx6",
        "outputId": "1bf90eeb-8626-4430-e0eb-57844f1374e6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMAPE Error: 25.66257482447715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters = {'criterion': 'squared_error',\n",
        "                  'max_depth': 3000, \n",
        "                  'min_samples_leaf': 7,\n",
        "                  'min_samples_split': 4,\n",
        "                  'n_estimators': 100}  # - submission(9) - 20.92%\n",
        "\n",
        "# random_forest = RandomForestRegressor(**rf_gs_model.best_params_)\n",
        "model3 = RandomForestRegressor(**best_parameters) \n",
        "model3.fit(trainX, train_y)\n",
        "check_error(model3.predict(testX), test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlXrUumHPX91",
        "outputId": "fc50d8b4-bce3-45ae-ca31-fa86936da3db"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMAPE Error: 25.837325684078206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters = {'criterion': 'squared_error',\n",
        "                  'max_depth': 3500,\n",
        "                  'min_samples_leaf': 7,\n",
        "                  'min_samples_split': 4,\n",
        "                  'n_estimators': 100}  # - submission(10) - 20.77%\n",
        "\n",
        "# random_forest = RandomForestRegressor(**rf_gs_model.best_params_)\n",
        "model4 = RandomForestRegressor(**best_parameters) \n",
        "model4.fit(trainX, train_y)\n",
        "check_error(model4.predict(testX), test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaTjOEnbPqLk",
        "outputId": "175f9d04-c529-4298-c9ab-6dfeda7d3a3f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMAPE Error: 25.73733541354521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters = {'criterion': 'squared_error',\n",
        "                  'max_depth': 4000, \n",
        "                  'min_samples_leaf': 7,\n",
        "                  'min_samples_split': 4,\n",
        "                  'n_estimators': 80}  # - submission(11) - 20.88%\n",
        "\n",
        "# random_forest = RandomForestRegressor(**rf_gs_model.best_params_)\n",
        "model5 = RandomForestRegressor(**best_parameters) \n",
        "model5.fit(trainX, train_y)\n",
        "check_error(model5.predict(testX), test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDXHQatDP0Ob",
        "outputId": "939aee55-8484-4eea-cc97-ee619a3abb70"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMAPE Error: 25.940215408068184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters = {'criterion': 'squared_error', \n",
        "                  'max_depth': 4000, \n",
        "                  'min_samples_leaf': 7,\n",
        "                  'min_samples_split': 4,\n",
        "                  'n_estimators': 100}  # - submission(12) - 20.85%\n",
        "\n",
        "# random_forest = RandomForestRegressor(**rf_gs_model.best_params_)\n",
        "model6 = RandomForestRegressor(**best_parameters) \n",
        "model6.fit(trainX, train_y)\n",
        "check_error(model6.predict(testX), test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzZqXxrYQAKw",
        "outputId": "19d78ddf-331b-470f-aece-7fb2b802e408"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMAPE Error: 26.055028981679033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters = {\n",
        "    'criterion': 'absolute_error', \n",
        "    'max_depth': 4000,  # 300\n",
        "    'n_estimators': 120, # 70\n",
        "    'min_samples_leaf': 7, # 3\n",
        "    'min_samples_split': 4} # 50\n",
        "\n",
        "model7 = RandomForestRegressor(**best_parameters) \n",
        "model7.fit(trainX, train_y)\n",
        "check_error(model7.predict(testX), test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzd_sVaNVh6E",
        "outputId": "7de5b34e-b983-4295-be46-fe2b3c88bf57"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMAPE Error: 19.777093325745685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Meta"
      ],
      "metadata": {
        "id": "KNGorFyBW_GL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainX_meta.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3JXWcuuakiq",
        "outputId": "0fcbb7d3-f63c-48f5-df4b-2b79227466a2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6625, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions1 = np.round(model1.predict(trainX_meta))\n",
        "predictions2 = np.round(model2.predict(trainX_meta))\n",
        "predictions3 = np.round(model3.predict(trainX_meta))\n",
        "predictions4 = np.round(model4.predict(trainX_meta))\n",
        "predictions5 = np.round(model5.predict(trainX_meta))\n",
        "predictions6 = np.round(model6.predict(trainX_meta))\n",
        "predictions7 = np.round(model7.predict(trainX_meta))\n",
        "train_samples = np.vstack((predictions1, predictions2, predictions3, predictions4, predictions5, predictions6, predictions7)).T\n",
        "print(train_samples.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mcFdUVpXqbM",
        "outputId": "2847e7e2-b042-498a-9d34-13f84fb231f3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6625, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lgr = LogisticRegression(max_iter=1000)\n",
        "# lgr.fit(train_samples, train_y_meta)\n",
        "best_parameters = {\n",
        "    'criterion': 'absolute_error', \n",
        "    'max_depth': 4500,  # 300\n",
        "    'n_estimators': 50, # 70\n",
        "    'min_samples_leaf': 7, # 3\n",
        "    'min_samples_split': 4} # 50\n",
        "\n",
        "meta_model = RandomForestRegressor(**best_parameters) \n",
        "meta_model.fit(train_samples, train_y_meta)\n",
        "# check_error(model7.predict(testX), test_y)\n",
        "predictions1 = np.round(model1.predict(testX_meta))\n",
        "predictions2 = np.round(model2.predict(testX_meta))\n",
        "predictions3 = np.round(model3.predict(testX_meta))\n",
        "predictions4 = np.round(model4.predict(testX_meta))\n",
        "predictions5 = np.round(model5.predict(testX_meta))\n",
        "predictions6 = np.round(model6.predict(testX_meta))\n",
        "predictions7 = np.round(model7.predict(testX_meta))\n",
        "test_samples = np.vstack((predictions1, predictions2, predictions3, predictions4, predictions5, predictions6, predictions7)).T\n",
        "print(test_samples.shape)\n",
        "check_error(meta_model.predict(test_samples), test_y_meta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ0BTOFeZRcG",
        "outputId": "15e75fce-c362-4691-e4ee-ff71d712dafd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMAPE Error: 19.777093325745685\n",
            "(737, 7)\n",
            "SMAPE Error: 21.906788247908672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions1 = np.round(model1.predict(val_test_x))\n",
        "predictions2 = np.round(model2.predict(val_test_x))\n",
        "predictions3 = np.round(model3.predict(val_test_x))\n",
        "predictions4 = np.round(model4.predict(val_test_x))\n",
        "predictions5 = np.round(model5.predict(val_test_x))\n",
        "predictions6 = np.round(model6.predict(val_test_x))\n",
        "predictions7 = np.round(model7.predict(val_test_x))\n",
        "val_samples = np.vstack((predictions1, predictions2, predictions3, predictions4, predictions5, predictions6, predictions7)).T\n",
        "predictions = np.round(meta_model.predict(val_samples))"
      ],
      "metadata": {
        "id": "zxA6CmMQXDEf"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_path = '/content/drive/MyDrive/Colab Notebooks/ML 4 course/nsu_sample_solution.csv'\n",
        "submission = pd.read_csv(submission_path)\n",
        "\n",
        "submission['Expected'] = predictions\n",
        "submission\n",
        "submission.to_csv('submission_meta.csv', index=False)"
      ],
      "metadata": {
        "id": "fJVryqS_W-dX"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Просто среднее предсказание лучших алгоритмов"
      ],
      "metadata": {
        "id": "bdzajqQEjyF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred1 = np.round(model1.predict(val_test_x))\n",
        "pred2 = np.round(model2.predict(val_test_x))\n",
        "pred3 = np.round(model3.predict(val_test_x))\n",
        "pred4 = np.round(model4.predict(val_test_x))\n",
        "pred5 = np.round(model5.predict(val_test_x))\n",
        "pred6 = np.round(model6.predict(val_test_x))\n",
        "pred7 = np.round(model7.predict(val_test_x))\n",
        "\n",
        "amount = 7\n",
        "print(len(best_pred))\n",
        "prediction = (pred1 + pred2 + pred3 + pred4 + pred5 + pred6 + pred7) / 7\n",
        "print(len(prediction))"
      ],
      "metadata": {
        "id": "eNdazc0Gyr8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1f117f3-625c-47de-f890-50cbfbbc8c26"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10741\n",
            "10741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_path = '/content/drive/MyDrive/Colab Notebooks/ML 4 course/nsu_sample_solution.csv'\n",
        "submission = pd.read_csv(submission_path)\n",
        "\n",
        "submission['Expected'] = np.round(prediction)\n",
        "submission\n",
        "submission.to_csv('best_submission.csv', index=False)"
      ],
      "metadata": {
        "id": "3HotpEjjy3q9"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}